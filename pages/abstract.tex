\chapter{\abstractname}

The amount of computing resources required to train state-of-the-art deep neural networks is steadily growing.
Most institutions cannot afford the latest technologies, which are sometimes needed to keep up with today's demanding deep neural network research.
Access to powerful devices is therefore limited to a few parties.
To tackle this, distributed training techniques such as volunteer computing can be employed.

Hivemind \cite{hivemind} is an open-source framework that enables collaborative model training using a large number of heterogeneous devices from universities, companies, and volunteers.
In Hivemind, every device participating in the computation may differ in its characteristics, featuring different architectures and network speeds.
The authors performed an interactive demonstration of collaborative computing with 40 volunteers, training a modified version of DALL-E \cite{ramesh2021zero} neural network model over 2 months.
The reported results, however, do not include information about the effects of different Hivemind configurations on training.

In this thesis, we focus on the effects of training ResNet18 \cite{he2015deep} on Imagenet-1k \cite{deng2009imagenet} with Hivemind when using the parameter averaging algorithm \cite{DBLP:journals/corr/abs-2106-10207} compared to regular training.
We evaluate the effect of several aspects of training with Hivemind in a controlled cluster setup through 288 synthetic experiments.
Each experiment uses different training and Hivemind settings such as the number of peers involved in the training, using local updates, batch size, learning rate.
We show that training with Hivemind can be beneficial in terms of training time in specific scenarios and settings compared to regular training with a single node.
Finally, we provide some considerations and lessons learned by summarizing the results of our experiments.
