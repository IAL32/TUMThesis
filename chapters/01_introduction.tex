% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{INTRODUCTION}\label{chapter:introduction}

It is safe to say that the internet paved the way for many things for humanity.
Media such as images, video and audio can be shared across websites and applications, knowledge can be stored on faraway servers.
This knowledge can then be retrieved with ease in text format using mobile devices, and products and services can be bought with the click of a button or a tap on a screen.
Interactions, media and information make up for massive amounts of data that flow through complex computer systems, which in turn generate even more data and information.

Researchers have found ways to leverage the magnitude of data that is being produced every second by countless systems all around the world.
One of the most recent and most popular uses of this huge variety and quantity of data is machine learning (ML).
Machine learning can be defined as a set of techniques that use data to improve performance in a set of tasks.
Today, for example, we feed data to machine learning models to calculate what is the probability that a webpage visitor will buy certain products, or the chances that it is going to rain in a few days or to generate elaborate text and stunning, never-before-seen pictures.

Recently, models such as BERT \cite{devlin2018bert}, DALL-E \cite{ramesh2021zero}, GPT-3 \cite{brown2020gpt3} and others have become incredibly popular thanks to their outstanding results and endless possibilities.
DALL-E for example can generate high-quality realistic images and art starting from a text description written in natural language.
These models however require massive amounts of data as well as very expensive computational resources, such as graphical processing units and tensor processing units (TPUs).
In recent years, the size of neural network models has been steadily increasing exponentially, as shown by \autoref{fig:model-size-over-time}.
A simple calculation shows that the neural network model Megatron-Turing-NLG 530B \cite{smith2022megatronturingnlg} would take roughly $530 \times 4 = 2120\texttt{GB}$ of memory to simply hold its 530 billion weights.

Furthermore, training a neural network model requires even more memory.
Intermediate computation outputs such as gradient and optimizer states sometimes require 2 or 3 times as much memory than just the model parameters, making GPU memory one of the main bottlenecks in training huge neural network models.
While some of these issues can be tackled using techniques such as parameter quantization \cite{DBLP:journals/corr/abs-2003-11316}, pruning and compression, they must not be considered one-fits-all solutions.
Some models are simply too big to be trained on a single device.
This problem is exacerbated by factors such as high GPU prices and much slower growth of their memory size relative to model size.
\autoref{fig:gpu-vram-over-time} shows how GPU memory has been increasing from 2016 to 2022.

\begin{figure}[h]
    \caption{GPU VRAM over the past 4 years. The growth is mostly linear, doubling }
    \label{fig:gpu-vram-over-time}
    \includegraphics[width=\textwidth]{./figures/gpu-vram-over-time.pdf}
\end{figure}

\begin{figure}[h]
    \caption{Model size over the past 4 years: ELMo \cite{peters2018elmo}, BERT \cite{devlin2018bert}, GPT-2 \cite{radford2019language}, Megatron-LM \cite{shoeybi2019megatronlm}, T-5 \cite{raffael2019t5}, Turing-NLG \cite{microsoft2020turingnlg}, GPT-3 \cite{brown2020gpt3}, Megatron-Turing-NLG \cite{smith2022megatronturingnlg}}
    \label{fig:model-size-over-time}
    \includegraphics[width=\textwidth]{./figures/model-size-over-time.pdf}
\end{figure}

To tackle the above challenges, practitioners studied and developed distributed computing techniques to train models that do not fit entirely in a single GPU's memory, distributing the training load to potentially thousands of devices.
One of the first, most prominent examples of distributed parallelism is the AlexNet network \cite{alexnet2012}, summarized in \autoref{fig:alexnet}.

\begin{figure}[h]
    \caption{AlexNet \cite{alexnet2012} architecture shows one of the first examples of model parallelism. The training of convolutional layers is split across two GPUs, as the size of the model during training exceeded the available memory of a single GPU.}
    \label{fig:alexnet}
    \includegraphics[width=\textwidth]{./figures/alexnet.pdf}
\end{figure}

Since then, techniques such as data and model parallelism have been widely adopted by the scientific community, leading to models with a huge number of parameters.
However, most of the frameworks and papers built on top of these techniques perform training using large amounts of expensive, top-of-the-art and homogeneous hardware.
This has become a problem for universities and small companies that want to train the models described in these papers, as they do not necessarily have access to a such vast amount of resources.

A solution to this issue is \textit{volunteer computing}, a paradigm in which people donate their idle computing resources such as laptops and other personal devices to solve hard problems.
The framework Hivemind \cite{riabinin2020hivemind} is a framework that enables volunteer computing across the internet for neural network training.
It implements two fundamental training algorithms, Moshpit SGD \cite{DBLP:journals/corr/abs-2103-03239} and DeDLOC \cite{DBLP:journals/corr/abs-2106-10207}.
They have successfully experimented with training a modified version of DALL-E with 40 peers \cite{learning30:online} who have donated their computing power through free resources such as Google Colab or their personal hardware.
However, the authors did not show the experiment results of using different settings of Hivemind.


Furthermore, training with smaller models such as ResNet18 \cite{he2015deep} and with a reliable network connection has not been tested.

In this thesis, we analyze the effects of several hyperparameters settings of Hivemind while training ResNet18.
We selected the ResNet18 \cite{he2015deep} model as our model of reference, trained on Imagenet-1k \cite{deng2009imagenet}.

\section{Motivation}

Training big neural network models is a big challenge in today's research, and access to the latest hardware makes this even more challenging.
Over the last few years, research and software libraries like Hivemind have been focused on reducing and optimizing deep neural network model training times with techniques such as data and model parallelism.
Hivemind has already been tested by its authors in a real-life scenario by successfully training a modified version of DALL-E over two months with 40 peers from all over the world \cite{DBLP:journals/corr/abs-2106-10207}
In particular, 
This thesis aims to fill that gap by performing 288 synthetic experiments on ResNet18 trained on Imagenet-1k with different configurations.
The insights gained from the results of this work can help broaden the applications of Hivemind.

Training model with large batches has recently gained popularity thanks to several works illustrating their props and cons \cite{DBLP:journals/corr/KeskarMNST16, you2017scaling, DBLP:journals/corr/abs-1904-00962}.
Hivemind

\section{Approach}

In this thesis, we will compare regular training using a single node with 16vCPUs to training using Hivemind with multiple peers, where the sum of vCPUs per peer always amounts to 16.
Also, the number of samples
We compare training with key Hivemind settings, namely:
\begin{itemize}
    \item Batch size;
    \item Learning rate;
    \item Number of peers involved in the training;
    \item Target number of samples that must be globally reached by all peers to perform an averaging round;
    \item Applying the gradients at every step or accumulating them until the next averaging round.
\end{itemize}

As we test the software and its limitations, we might find possible areas of improvement in Hivemind.
Whenever possible, we will further contribute using the knowledge gathered through our experiments by improving the Hivemind \cite{hivemind} source code.

\section{Contributions}

Our contributions are as follows:
\begin{itemize}
    \item We analyze the challenges of optimizing preprocessing pipelines in decentralized distributed training and provide insights on possible improvements
    \item We verify the effectiveness of Hivemind for different peer hardware configurations concerning preprocessing pipelines
    \item We use the gained knowledge and insights to contribute to the Hivemind open-source library.
\end{itemize}
