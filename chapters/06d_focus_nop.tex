\section{FOCUS ON EFFECTS OF THE NUMBER OF PEERS AND vCPUS PER PEER}

Institutions and companies may have more than two machines at their disposal to perform distributed training.
So far, we have explored the effects on Hivemind of specific settings such as TBS, BS LR, GAS and LU.
Adding more nodes to a distributed training setting can lead to bottlenecks, especially when using a client-server approach \cite{Atre_2021, 8886576}.
In this section, we answer the following research question: what are the effects of scaling up the number of machines when using Hivemind?

\begin{figure}[ht]
    \centering
    % temporary
    \foreach \gas in {1, 2}
        {
            \begin{subfigure}[t]{0.45 \textwidth}
                \caption{}
                \includegraphics[width=\textwidth]{./figures/06_barplot-runtime_gas-\gas_scale-nop.pdf}
            \end{subfigure}
        }
    \caption{
        Runtime (bars) in minutes for Hivemind runs with 2, 4, 8, 16 peers and 8, 4, 2, 1 vCPUs respectively.
        Runs are aggregated across LR and the standard error amongst runs is shown with black bars.
        The black dashed line indicates the throughput in samples per second.
    }
    \label{fig:runtime-decrease_scale-nop}
\end{figure}

The frequency at which peers average their model state is directly proportional to the number of peers, the throughput per second of each peer and the TBS.
In turn, the throughput per second is affected by several factors such as the BS, computational power of the node and wait times for I/O operations.

It might be difficult to isolate the effects of introducing more nodes from scaling the target batch size.
Thus, we decided to fix the target batch size to 1250 for this set of experiments and alter BS, LR, GAS and LU.

\begin{figure}[ht]
    \centering
    \foreach \gas in {1}
        {
            \foreach \lu in {False, True}
                {
                    \begin{subfigure}[t]{0.45 \linewidth}
                        \centering
                        \caption{}
                        \includegraphics[width=\textwidth]{./figures/06_barplot-loss_gas-\gas_lu-\lu_scale-nop.pdf}
                    \end{subfigure}
                }
        }
    \caption{GAS = 1, accuracy decrease in percent for Hivemind runs with 2, 4, 8, 16 peers and 8, 4, 2, 1 vCPUs respectively relative to baseline runs. Higher is worse.}
    \label{fig:loss-increase_scale-nop}
\end{figure}

\begin{figure}[ht]
    \centering
    \foreach \gas in {2}
        {
            \foreach \lu in {False, True}
                {
                    \begin{subfigure}[t]{0.45 \linewidth}
                        \centering
                        \caption{}
                        \includegraphics[width=\textwidth]{./figures/06_barplot-loss_gas-\gas_lu-\lu_scale-nop.pdf}
                    \end{subfigure}
                }
        }
    \caption{GAS = 2, accuracy decrease in percent for Hivemind runs with 2, 4, 8, 16 peers and 8, 4, 2, 1 vCPUs respectively relative to baseline runs. Higher is worse.}
\end{figure}

As we might expect, \autoref{fig:runtime-decrease_scale-nop} shows that increasing the number of peers dramatically decreases runtime.
The highest jump in runtime performance is between using one single peer (Hivemind disabled) and using two peers (Hivemind enabled).
Introducing four peers also cuts down runtime by around 50\% compared to using two peers across all experiments.
However, this effect does not appear to be linear.
The benefits of including more peers only increase by 10-15\% for eight peers and 4-6\% for sixteen peers.
If we take into consideration the decreased accuracy performance, there seems to be a sweet spot in terms of reducing the total runtime and an acceptable reduction in accuracy performance.
Using four peers seems to be the optimal number of peers when training with Hivemind on our configuration to obtain the maximum reduction of runtime without having a significant hit in terms of accuracy.
It remains an open question whether training these runs for longer would yield the same accuracy as the baseline runs but in less time overall.
Further experimentation may also show that increasing the TBS, and thus reducing the averaging frequency amongst peers, can be beneficial in runs where TBS is quickly reached.

\autoref{fig:loss-increase_scale-nop} shows that GAS and LU settings seem to generally have a similar effect compared to Hivemind runs with 2 peers and 8vCPUs presented in \autoref{sec:focus-effect-bs-lr-tbs}.
The graph also shows us a decrease in performance as we increase the number of peers, especially for experiments that have reached a higher accuracy.
In general, we noticed that compared to baseline experiments with bad performance, the accuracy does not change too much when using Hivemind.

\begin{figure}[h]
    \centering
    \foreach \gas in {1, 2}
        {
            \begin{subfigure}[t]{0.45\textwidth}
                \centering
                \caption{}
                \includegraphics[width=\textwidth]{./figures/06_barplot-times_gas-\gas_scale-nop.pdf}
            \end{subfigure}%
        }
    \caption{
        Cumulative time taken by the \texttt{opt.step()} for Hivemind runs with 2, 4, 8, 16 peers and 8, 4, 2, 1 vCPUs respectively in minutes.
        Runs are further aggregated across LR.
    }
    \label{fig:times-stacked_scale-nop}
\end{figure}%

\autoref{fig:times-stacked_scale-nop} shows the cumulative time in minutes taken by the optimization step for the Hivemind experiments changing NoP.
We can notice little changes when increasing from GAS=1 to GAS=2 when local updates are disabled, with a slight increase in processing time as we increase the NoP.
This is to be expected, as the more peers are introduced, the more often peers can perform averaging.
However, this effect is dampened by the simultaneous reduction in the processing speed of every node.

In \autoref{chapter:setup} we described the basic setup and introduced the Hivemind setting \texttt{MATCHMAKING\_TIME}, which controls how long peers should wait for other participants until they start an averaging round.
Hivemind's documentation suggests decreasing this value to 3-5 seconds if training runs are small and performed locally and increasing this value to 10-15 seconds if training over the internet or with many peers.
However, we found that practitioners should also take how much time each step takes to complete, as this directly impacts how long peers wait for the averaging step and how many peers they average with.
In the previous sections, we have shown that the frequency at which peers perform the averaging step directly impacts the final model's accuracy.
Thus, maximizing the probability that peers will find all averaging partners can lead to much better results.
In a local and controlled setup, if the step time is too high compared to the matchmaking time, peers will wait for too little, potentially averaging with few or no peers at all.
If the step time is too low compared to the matchmaking time, peers will face the opposite issue and will waste time waiting too long for other peers.
Future work may want to study an optimization strategy targeting dynamic changes in matchmaking time to respond better to peers joining a training network.


\begin{figure}[h]
    \centering
    \foreach \gas in {1, 2}
        {
            \begin{subfigure}[t]{0.45\linewidth}
                \centering
                \caption{}
                \includegraphics[width=\textwidth]{./figures/06_net-recv-sys-bandwidth-mbs_gas-\gas_scale-nop.pdf}
            \end{subfigure}%
        }
    \caption{Network received for Hivemind runs with 2, 4, 8, 16 peers and 8, 4, 2, 1 vCPUs respectively. Values $\geq 20$ MB/s are hidden and runs are aggregated across LR.}
    \label{fig:net-recv-sys-bandwidth-mbs_scale-nop}
\end{figure}%
\begin{figure}[h]
    \centering
    \foreach \gas in {1, 2}
        {
            \begin{subfigure}[t]{0.45\linewidth}
                \centering
                \caption{}
                \includegraphics[width=\textwidth]{./figures/06_net-sent-sys-bandwidth-mbs_gas-\gas_scale-nop.pdf}
            \end{subfigure}%
        }
    \caption{Network sent for Hivemind runs with 2, 4, 8, 16 peers and 8, 4, 2, 1 vCPUs respectively. Values $\geq 20$ MB/s are hidden and runs are aggregated across LR.}
    \label{fig:net-sent-sys-bandwidth-mbs_scale-nop}
\end{figure}

When local updates are enabled, the difference between GAS=1 and GAS=2 are instead much more noticeable, with GAS=2 drastically reducing the time spent on the optimization step.
This may be because peers call less frequently \texttt{opt.step()}, they perform fewer averaging rounds overall.
As the number of peers increases, we also see a reduction in time spent.
This is also due to the reduced number of maximum steps called, which becomes half again if we increase GAS.

The network bandwidth utilization for different peer configurations is shown in \autoref{fig:net-recv-sys-bandwidth-mbs_scale-nop} and \autoref{fig:net-sent-sys-bandwidth-mbs_scale-nop}, and yield interesting results.
As we increase the number of peers in a training session, peers communicate more often, which can also be seen as these "bulbs" in the violin plots.
This is unsurprising, for two reasons:
first, the time to reach the fixed TBS of 1250 gets shorter as we add more peers, thus, the frequency at which peers communicate increases with the number of peers;
second, with more peers to average with, there is more data to exchange in terms of pings, synchronization messages and such.
We can see that with sixteen peers, the sent bandwidth utilization is almost exclusively around 5MB/s.
Nevertheless, even with sixteen peers, we did not see any significant CPU bottleneck caused by high network communication.

Finally, we make different observations for GAS and LU values as we presented in \autoref{sec:focus-local-updates} and \autoref{sec:focus-gradient-acc}.
Enabling LU for a higher number of peers seems to be very penalizing, especially with NoP=16.
Disabling LU instead yields more consistent accuracies in every experiment but seems to perform best for large values of LR.
Increasing GAS appears to worsen the situation when paired with LU=True, except for very low values of LR.
On the other hand, increasing GAS to 2 while disabling local updates seems to be a bit better, but not as consistently as decreasing TBS in \autoref{sec:focus-effect-bs-lr-tbs}.

Summarizing the findings, we can say the following for our setup:
\begin{itemize}
    \item Increasing the number of peers while maintaining the same computational power can reduce the total runtime by at least 30\%.
    \item However runtime reduction is not linear compared to the number of peers.
          The effects of reducing the data load times by using faster storage are still an open question.
    \item With local updates enabled, increasing the number of peers seems to have a worse effect on training accuracy.
          The effects of other values of TBS for a different number of peers is still an open question.
    \item Introducing more peers leads to more bandwidth usage as each peer exchanges more data with other averaging partners.
          This effect can become much larger for larger models and a much larger number of nodes.
    \item At the same time, increasing the number of peers reduces the amount of time spent averaging given a fixed sample budget.
    \item When enabling local updates, it is important to increase or decrease the Hivemind setting \texttt{MATCHMAKING\_TIME} to a higher value than the time it takes for peers to go through data load, forward and backward passes.
          However, this value should not be too high, otherwise, peers will waste time waiting too long for other peers to show up.
\end{itemize}
