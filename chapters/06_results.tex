\chapter{Results}\label{chapter:results}

In this thesis, we are not looking to obtain the best possible combination of hyperparameters for training loss or model accuracy.
Instead, we want to observe the effects on training with Hivemind when tuning common hyperparameters such as batch size and learning rate and Hivemind hyperparameters such as the TBS.
In this thesis, we analyze the performance and limits of training using Hivemind rather than looking for the best model.

\section{Baseline runs}

We begin this chapter by showing the results that we have obtained with the baseline runs.
As mentioned previously in \autoref{chapter:setup}, all baseline experiments are executed on machines with the same configuration, and the total number of samples processed is always the same.
\autoref{fig:baseline-runtimes} shows the average runtimes for baseline runs in minutes.

\begin{figure}[h]
    \centering
    \foreach \gas in {1, 2}
        {
            \begin{subfigure}[b]{0.475\textwidth}
                \centering
                \caption{}
                \includegraphics[width=\textwidth]{./figures/06_barplot-runtime_gas-\gas_baseline-16vCPUs-GAS-1.pdf}
            \end{subfigure}%
            \hfill
        }
    \caption{Average runtimes of baseline experiments in minutes. Runs are aggregated across LR, with the standard deviation amongst reruns as the black bar.}
    \label{fig:baseline-runtimes}
\end{figure}

\begin{figure}[h]
    \centering
    \foreach \gas in {1, 2}
        {
            \begin{subfigure}[b]{\textwidth}
                \centering
                \caption{}
                \includegraphics[width=\textwidth]{./figures/06_barplot-losses_gas-\gas_baseline-16vCPUs-GAS-1.pdf}
            \end{subfigure}%
            \hfill
        }
    \caption{Loss achieved by baseline runs, averaged across re-runs.}
    \label{fig:baseline-losses}
\end{figure}

Baseline runs do not run distributed algorithms, all Hivemind features are switched off and machines do not communicate with each other.
However, \autoref{fig:net-recv_baseline} shows that there is some network activity.
On average, every machine receives a constant 1.5 MB/s of data on its network.
This may be due to several factors, such as KVM management data, OpenNebula pings, and CEPH data being read.

In the Setup section, we also introduced our monitoring tool of choice \textit{wandb}.
Because this is an online monitoring tool, some data about our runs is periodically sent to the Weights and Biases server for storage and visualizations.

In \autoref{fig:net-sent_baseline}, which shows the bandwidth used for send operations across all baseline runs, we can observe the bandwidth in MB/s used for each run.
On average, this is roughly 0.02 MB/s on every run, a value that can be mostly attributed to \textit{wandb} and other background monitoring operations such as OpenNebula.

In future sections, we will always account for these effects when performing comparisons with baseline runs.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.475 \textwidth}
        \centering
        \caption{Network bandwidth sent in MB/s for baseline runs. Values above 0.07 are hidden. Runs are aggregated across LR.}
        \label{fig:net-sent_baseline}
        \includegraphics[width=\textwidth]{./figures/06_net-recv_baseline-16vCPUs-GAS-1.pdf}
    \end{subfigure}%
    \hfill
    \centering
    \begin{subfigure}[b]{0.475 \textwidth}
        \centering
        \caption{Network bandwidth received in MB/s for baseline runs. Values above 5 are hidden. Runs are aggregated across LR.}
        \label{fig:net-recv_baseline}
        \includegraphics[width=\textwidth]{./figures/06_net-sent_baseline-16vCPUs-GAS-1.pdf}
    \end{subfigure}%
    \hfill
    \caption{Network bandwidth sent and received in MB/s for baseline runs. Runs are aggregated across LR.}
\end{figure}

\autoref{fig:baseline-times-stacked} shows the average times for data load, forward pass, backward pass and optimization step across batch sizes in baseline runs for both GAS=1 and GAS=2.
As we might expect, the time it takes for a single step to complete is linearly dependent on the batch size.
The learning rate (LR) does not affect the time it takes for each step to complete, so we aggregated the runs for each batch size.
By contrast, the number of gradient accumulation steps (GAS) seems to shave off some time for every batch size, although the total runtimes in \autoref{fig:baseline-runtimes} do not seem to reflect this improvement.
Throughout this chapter, we will keep showing GAS runs separately, as it still might affect some other aspects of training.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{./figures/06_barplot-times_baseline-16vCPUs-GAS-1.pdf}
    \caption{
        Average times of step data load (small circles), forward pass (backward slash), backward pass (forward slash) and optimization step (stars) baseline experiments in seconds.
        Runs are further aggregated across LR and the standard error amongst runs is shown with black bars.
    }
    \label{fig:baseline-times-stacked}
\end{figure}

\input{chapters/06a_focus_batch_size.tex}

\section{Focus on effects of gradient accumulation}

Gradient accumulation allows the simulation of bigger batches within a single node by accumulating gradients every time the backpropagation step is performed.
After GAS steps, the optimizer step is performed and the gradients are finally applied to the trained model.
In theory, reducing the frequency of executing the optimizer step should also reduce the total time spent applying the gradients to the mode.
In practice, for small models like ResNet18, this doesn't make a discernible difference as shown in \autoref{fig:times-stacked_2-peers-8vCPUs}, where the optimizer step takes 0.05 seconds on average.

For loss, the scenario is quite different.
In \autoref{fig:loss-increase_2-peers-8vCPUs} we can see the loss increase with respect to the baseline runs in four different configurations:
\begin{itemize}
    \item GAS=1, LU=True; 
    \item GAS=1, LU=False;
    \item GAS=2, LU=True;
    \item GAS=2, LU=False;
\end{itemize}

With both LU=True and LU=False, we can notice a better loss with GAS=2 by 5-10\% compared to GAS=1 for experiments with high lower LR.
As LR increases, the gap between GAS=1 and GAS=2 closes, with the gap getting even closer for smaller TBS values.

Finally, the impact on network utilization using our configuration is minimal.
Possibly for scenarios with more traffic, high values of GAS may help reduce the number of times that the Hivemind optimizer is called, reducing step time.

Evaluating the benefits of using gradient accumulation and averaging, we can say the following when training ResNet18 on Imagenet with Hivemind:
\begin{itemize}
    \item the smaller the TBS, the less the difference between GAS=1 and GAS=2 matters. It remains an open question whether this statement holds for higher values of GAS.
    \item for high values of LR, GAS does not seem to affect training as much as for low values of LR;
\end{itemize}

\section{Focus on effects of local updates}

\input{chapters/06d_focus_nop.tex}
