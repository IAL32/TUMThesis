\chapter{RESULTS}\label{chapter:results}

In this thesis, we are not looking to obtain the best possible combination of hyperparameters for training loss or model accuracy.
Instead, we want to observe the effects on training with Hivemind when tuning common hyperparameters such as batch size and learning rate and Hivemind hyperparameters such as the TBS.
In this thesis, we analyze the performance and limits of training using Hivemind rather than looking for the best model.

\section{Baseline runs}

We begin this chapter by showing the results that we have obtained with the baseline runs.
As mentioned previously in \autoref{chapter:setup}, all baseline experiments are executed on machines with the same configuration, and the total number of samples processed is always the same.
\autoref{fig:baseline-runtimes} shows the average runtimes for baseline runs in minutes.

\begin{figure}[h]
    \centering
    \foreach \gas in {1, 2}
        {
            \begin{subfigure}[b]{0.475\textwidth}
                \centering
                \caption{}
                \includegraphics[width=\textwidth]{./figures/06_barplot-runtime_gas-\gas_baseline-16vCPUs-GAS-1.pdf}
            \end{subfigure}%
            \hfill
        }
    \caption{Average runtimes of baseline experiments in minutes. Runs are aggregated across LR, with the standard deviation amongst reruns as the black bar.}
    \label{fig:baseline-runtimes}
\end{figure}

\begin{figure}[h]
    \centering
    \foreach \gas in {1, 2}
        {
            \begin{subfigure}[b]{\textwidth}
                \centering
                \caption{}
                \includegraphics[width=\textwidth]{./figures/06_barplot-losses_gas-\gas_baseline-16vCPUs-GAS-1.pdf}
            \end{subfigure}%
            \hfill
        }
    \caption{Loss achieved by baseline runs, averaged across re-runs.}
    \label{fig:baseline-losses}
\end{figure}

Baseline runs do not run distributed algorithms, all Hivemind features are switched off and machines do not communicate with each other.
However, \autoref{fig:net-recv_baseline} shows that there is some network activity.
On average, every machine receives a constant 1.5 MB/s of data on its network.
This may be due to several factors, such as KVM management data, OpenNebula pings, and CEPH data being read.

In the Setup section, we also introduced our monitoring tool of choice \textit{wandb}.
Because this is an online monitoring tool, some data about our runs is periodically sent to the Weights and Biases server for storage and visualizations.

In \autoref{fig:net-sent_baseline}, which shows the bandwidth used for send operations across all baseline runs, we can observe the bandwidth in MB/s used for each run.
On average, this is roughly 0.02 MB/s on every run, a value that can be mostly attributed to \textit{wandb} and other background monitoring operations such as OpenNebula.

In future sections, we will always account for these effects when performing comparisons with baseline runs.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.475 \textwidth}
        \centering
        \caption{Network bandwidth sent in MB/s for baseline runs. Values above 0.07 are hidden. Runs are aggregated across LR.}
        \label{fig:net-sent_baseline}
        \includegraphics[width=\textwidth]{./figures/06_net-recv_baseline-16vCPUs-GAS-1.pdf}
    \end{subfigure}%
    \hfill
    \centering
    \begin{subfigure}[b]{0.475 \textwidth}
        \centering
        \caption{Network bandwidth received in MB/s for baseline runs. Values above 5 are hidden. Runs are aggregated across LR.}
        \label{fig:net-recv_baseline}
        \includegraphics[width=\textwidth]{./figures/06_net-sent_baseline-16vCPUs-GAS-1.pdf}
    \end{subfigure}%
    \hfill
    \caption{Network bandwidth sent and received in MB/s for baseline runs. Runs are aggregated across LR.}
\end{figure}

\autoref{fig:baseline-times-stacked} shows the average times for data load, forward pass, backward pass and optimization step across batch sizes in baseline runs for both GAS=1 and GAS=2.
As we might expect, the time it takes for a single step to complete is linearly dependent on the batch size.
The learning rate (LR) does not affect the time it takes for each step to complete, so we aggregated the runs for each batch size.
By contrast, the number of gradient accumulation steps (GAS) seems to shave off some time for every batch size, although the total runtimes in \autoref{fig:baseline-runtimes} do not seem to reflect this improvement.
Throughout this chapter, we will keep showing GAS runs separately, as it still might affect some other aspects of training.

In the same graph, we can also notice the big impact that data loading has on every step.
Almost 1/2 of the total time for each step consists in waiting for the data to load.
As we increase the number of cores per peer, CPU utilization decreases, as the CPU is idle during I/O wait times and normal operations such as forward and backward pass take less time.
This is a bottleneck that can easily be tackled through several means, such as having a faster storage backend or if that is not available, faster data loader frameworks and algorithms \cite{isenko2022bottleneck, leclerc2022ffcv}.
Future experiments can make use of local, faster storage backed by SSD to achieve faster data load speeds, helping us rule out the effects of data loading on training with Hivemind.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{./figures/06_barplot-times_baseline-16vCPUs-GAS-1.pdf}
    \caption{
        Average times of step data load (small circles), forward pass (backward slash), backward pass (forward slash) and optimization step (stars) baseline experiments in seconds.
        Runs are further aggregated across LR and the standard error amongst runs is shown with black bars.
    }
    \label{fig:baseline-times-stacked}
\end{figure}

\input{chapters/06a_focus_batch_size.tex}

\input{chapters/06b_focus_gradient_acc.tex}

\input{chapters/06c_focus_local_updates.tex}

\input{chapters/06d_focus_nop.tex}
