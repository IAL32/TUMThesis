\section{Focus on effects of local updates}\label{sec:focus-local-updates}
By default, the Hivemind Optimizer wraps around a Pytorch optimizer, taking control of underlying actions such as the application of gradients to the underlying model.
When local updates (LU) are enabled, gradients are applied directly to the model at each call of the Hivemind Optimizer.
When LU are disabled, the gradients are only applied to a model after the gradient averager and state averager have finished.

Taking a look at \autoref{fig:runtime-decrease_2-peers-8vCPUs}, we can notice that the runtime difference between enabling or disabling local updates is minimal and perhaps not relevant.
As we can notice by the very low bandwidth usage in both \autoref{fig:net-recv-sys-bandwidth-mbs_2-peers-8vCPUs} and \autoref{fig:net-sent-sys-bandwidth-mbs_2-peers-8vCPUs}, LU also has little effect on networking for our setup.
This may be due to several factors, such as the relatively small size of the model and the optimizer

In \autoref{fig:loss-increase_gas-1_2-peers-8vCPUs} and \autoref{fig:loss-increase_gas-2_2-peers-8vCPUs} the difference between the two modes in terms of accuracy decrease compared to the baseline is quite noticeable.
Disabling local updates consistently leads to worse performance compared to the baseline experiments with the highest accuracy.
However, for low accuracy increase, the results are not entirely relevant.
The final accuracy is still too low to be considered a good training result compared to the baseline experiments.

For both GAS=1 and GAS=2, we can observe that the penalty for disabling local updates with large values of target batch size is very big.
As we increase the TBS, this penalty goes up even further, sometimes more than 50\% compared to enabling local updates.
As previously stated, for small runs such as the ones presented in this thesis, the impact of using local updates is virtually negligible, and thus can be preferred.
It is currently an open question if this will hold for larger models and a higher number of peers.

Increasing GAS also yields interesting results for LU=True.
Because GAS=2 prevents updating the underlying model for one step, we can see the negative effects of enabling local updates, which rely on applying the gradients at every step to perform averaging between all peers.
We can see this effect especially for larger LR values.
This can have serious implications when simulating bigger batch sizes, as this is usually done by accumulating gradients and thus increasing GAS.

In short, this is what we learned from the effects of local updates:
\begin{itemize}
    \item Enabling local updates seems to work best at virtually no cost to overall performance using the setup presented in this thesis.
    \item Disabling local updates is more unforgiving in terms of the accuracy decrease, with additional penalties as the target batch size increases and thus the waiting time between averaging rounds.
          As long as peers can communicate as often as possible however this does not seem to be an issue.
    \item Increasing GAS with local updates enabled may cause worse performance in terms of accuracy, taking into account baseline runs with good performance.
    \item On the contrary, with disabled local updates, GAS=2 leads to overall better accuracy compared to GAS=1, but it is still bad with respect to baseline runs.
\end{itemize}
