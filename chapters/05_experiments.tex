\chapter{EXPERIMENTS}\label{chapter:experiments}

Having access to more powerful hardware is a challenge for several reasons that may be outside of our control.
In the past years, there was a worldwide shortage of microchips that negatively impacted the ability to purchase state-of-the-art hardware such as CPUs and GPUs.
Thus, we would like to understand the effects of running distributed frameworks such as Hivemind on less powerful, older hardware.

To provide a fair comparison between experiments not running Hivemind and experiments that do, our experiments always have the same amount of vCPUs.
Finally, every experiment processes the same number of samples across all the participating peers, and the sum of processed samples may never be greater than 320,000.

An exception to this rule is made for experiments with an odd number of samples per peer.
For example, a run with batch size $128$ and 8 peers should result in 312.5 samples per peer, which is not possible.
In these cases, the number of samples per peer is rounded up to the nearest digit to form an even number.
This chapter describes the basic setup of our experiments.

\section{BASELINE CASE}

To preserve a comparison consistency between each experiment run, the number of steps depends on two factors: the number of peers involved in the training, and the batch size.
We designed our baseline experiments in a grid search, covering the following training hyperparameters:
\begin{itemize}
    \item Batch Size (BS): 32, 64 and 128;
    \item Learning Rate (LR): 0.001, 0.01 and 0.1;
    \item Max Steps (MS): 10000 for BS=32, 5000 for BS=64, 2500 for BS=128.
    \item Gradient Accumulation Steps (GAS): 1 (no accumulation), 2 (with accumulation up to two steps)
\end{itemize}

\begin{tabularx}{\linewidth}{ |p{3cm}|p{3cm}|p{3cm}|p{3cm}|  }
    \caption{
        List of baseline experiments and hyperparameters
    }\label{table:baseline-experiments}                       \\
    \hline
    \multicolumn{4}{|c|}{Baseline experiments}                \\
    \hline
    Max Steps & Batch Size & Learning Rate & Grad. Acc. Steps \\
    \hline
    10000, 5000, 2500    & 32, 64, 128         & 0.001, 0.01, 0.1         & 1, 2                \\
    \hline
\end{tabularx}

The machines used for baseline runs have 16vCPUs and each experiment is repeated 4 times to observe the reproducibility of the measurements.
Hivemind features such as the DHT and the Optimizer wrapper are completely deactivated for these runs.
\autoref{table:baseline-experiments} lists all the 18 combinations of experiments that we cover in this thesis.

\section{NOT-BASELINE CASE}\label{sec:not-baseline-case}

To test and isolate the effects of using Hivemind for distributed training, every experiment changes only a single parameter at a time.
For this, we can divide the set of non-baseline cases into different categories depending on which parameter has been changed.
In every non-base case scenario described in this section, at least two nodes are involved in the training of the underlying NN model.

The model and the dataset remain the same across each run, and every peer has full access to the entire dataset through our CEPH cluster.
We repeat the same experiments as the baseline runs, and further explore the following Hivemind settings and questions:
\begin{itemize}
    \item \textbf{Number of Peers (NoP)}: 2, 4, 8 and 16; for loads like the experiment that we are running, is communication between many nodes a bottleneck?
    \item \textbf{Number of logical cores per node (vCPUs)}: 1, 2, 4, 8 and 16; using the same amount of computational power across many nodes, do we get to a target accuracy faster?
    \item \textbf{Target Batch Size (TBS)}: 10000, 5000, 2500, 1250 and 625; using smaller target batch size, do we get faster to a target accuracy?
    \item \textbf{Max Steps (MS)}: 5000, 2500, 1250 and 625; this parameter depends on the number of peers and batch size, but the total is always 320,000 steps per experiment;
    \item \textbf{Use Local Updates (LU)}: \texttt{True} or \texttt{False}; Hivemind allows us to control when to schedule gradient, model and parameter averaging. How does this setting affect training?
\end{itemize}

The \autoref{table:hivemind-experiments} shows a list of the combination of experiments that we performed to test Hivemind.
In total, we have executed 288 experiments for this thesis.

\begin{tabularx}{\linewidth}{ |c|c|c|c|c|c|c|c|  }
    \caption{
        List of Hivemind experiments and hyperparameters.
        Every experiment has been executed once, and every time with at least two peers.
    }\label{table:hivemind-experiments}                                                      \\
    \hline
    \multicolumn{8}{|c|}{Experiments testing for the effect of target batch size (TBS)}      \\
    \hline
    MS   & NoP & vCPUs & BS  & LR               & TBS                           & GAS & LU   \\
    \hline
    5000 & 2   & 8     & 32  & 0.001, 0.01, 0.1 & 10000, 5000, 2500, 1250, 625 & 1,2 & T, F \\
    2500 & 2   & 8     & 64  & 0.001, 0.01, 0.1 & 10000, 5000, 2500, 1250, 625 & 1,2 & T, F \\
    1250 & 2   & 8     & 128 & 0.001, 0.01, 0.1 & 10000, 5000, 2500, 1250, 625 & 1,2 & T, F \\
    \hline
    \multicolumn{8}{|c|}{Experiments testing for the effect of the number of peers (NoP)}    \\
    \hline
    MS   & NoP & vCPUs & BS  & LR               & TBS                           & GAS & LU   \\
    \hline
    2500 & 4   & 4     & 32  & 0.001, 0.01, 0.1 & 1250                          & 1,2 & T, F \\
    1250 & 8   & 2     & 32  & 0.001, 0.01, 0.1 & 1250                          & 1,2 & T, F \\
    625  & 16  & 1     & 32  & 0.001, 0.01, 0.1 & 1250                          & 1,2 & T, F \\
    1250 & 4   & 4     & 64  & 0.001, 0.01, 0.1 & 1250                          & 1,2 & T, F \\
    625  & 8   & 2     & 64  & 0.001, 0.01, 0.1 & 1250                          & 1,2 & T, F \\
    313  & 16  & 1     & 64  & 0.001, 0.01, 0.1 & 1250                          & 1,2 & T, F \\
    625  & 4   & 4     & 128 & 0.001, 0.01, 0.1 & 1250                          & 1,2 & T, F \\
    313  & 8   & 2     & 128 & 0.001, 0.01, 0.1 & 1250                          & 1,2 & T, F \\
    157  & 16  & 1     & 128 & 0.001, 0.01, 0.1 & 1250                          & 1,2 & T, F \\
    \hline
\end{tabularx}

We have fixed the following Hivemind hyperparameters that were not the focus of this thesis, although some can be further explored:
\begin{itemize}
    \item \textbf{matchmaking\_time}; defines for how many seconds the optimizer should wait for other peers to join an averaging round.
          We set this value to 10.
          Ideally, optimizers should never have to wait for this long amount of time in our setup.
    \item \textbf{averaging\_timeout}; after this many seconds, an averaging round is canceled.
          We set this value to 300.
          This high value is not encouraged by the Hivemind framework, as it may cause optimizers to hang in case of network errors.
          However, because we have a controlled environment with low latency, setting this to a high value allows us to quickly determine issues with our setup and intervene by re-running the experiments.
    \item \textbf{grad\_compression}; defines which class to use for gradient compression.
          We set this to \texttt{hivemind.NoCompression} for every run, as exploring the effects of compression for gradients is outside the focus of this thesis.
          Other works have focused on the effects of data sparsity and data parallelism \cite{DBLP:journals/corr/abs-2003-11316}.
\end{itemize}

\section{METRICS COMPARISON FRAMEWORK}

In the next chapter, we will compare training and system metrics between baseline and Hivemind runs.
However, they are not directly comparable.

Hivemind runs involve more than one machine per experiment, with each machine completing its task earlier or later than other peers that are in the same training network.
We assume that an experiment ends when the last peer finishes processing the samples it has been assigned.
Thus, for Hivemind runs we chose to use the maximum runtime amongst all peers to be used for comparison and analysis.

To compare baseline and Hivemind runs, we selected training accuracy as the metric of choice for three reasons.
First, previous work on distributed training optimization tends to use accuracy to report their improvements, thus, using accuracy ourselves helps make better comparisons with other works.
Second, accuracy is a function of the number of errors done across batches, while loss is a distance function that determines how big these errors are.
This means that using accuracy is less subjective to which distance function we select, allowing for even broader comparisons for other works.
Finally, training loss can be very unstable, potentially leading to very low values of loss at random, while accuracy is much more stable in that regard.

For training accuracy, we always select the average between the maximum training accuracy reached by the baseline re-runs, and the maximum accuracy reached by all peers for Hivemind runs.
We chose the maximum for the Hivemind runs because the model being trained is essentially one.
When saving the model for inference purposes, the selection of which peer's model to pick should lie on the model with the maximum loss achieved.

We use the average for system metrics such as bandwidth received and sent, CPU load and RAM usage for both baseline and Hivemind runs.
This is because all nodes in our controlled Hivemind experiments behave more or less the same.
In real-life scenarios such as training across the internet, where latency and peer behavior is unpredictable, this assumption would not be possible.

The throughout in samples per second metric presented in the next chapter is aggregated across re-runs for baselines and peers for Hivemind experiments.
This metric does not take into account failed average attempts, which synchronizes the peer's state with another peer, effectively nullifying its contribution up until that point.
Future experiments using this setup should consider this and not include a peer's contribution toward the throughput metric if this happens.
