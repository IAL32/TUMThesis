@book{latex,
  title     = {LaTeX : A Documentation Preparation System User's Guide and Reference Manual},
  publisher = {Addison-Wesley Professional},
  year      = {1994},
  author    = {Leslie Lamport}
}

@inproceedings{isenko2022bottleneck,
  author    = {Isenko, Alexander and Mayer, Ruben and Jedele, Jeffrey and Jacobsen, Hans-Arno},
  title     = {Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines},
  year      = {2022},
  isbn      = {9781450392495},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3514221.3517848},
  doi       = {10.1145/3514221.3517848},
  abstract  = {Preprocessing pipelines in deep learning aim to provide sufficient data throughput to keep the training processes busy. Maximizing resource utilization is becoming more challenging as the throughput of training processes increases with hardware innovations (e.g., faster GPUs, TPUs, and inter-connects) and advanced parallelization techniques that yield better scalability. At the same time, the amount of training data needed in order to train increasingly complex models is growing. As a consequence of this development, data preprocessing and provisioning are becoming a severe bottleneck in end-to-end deep learning pipelines.In this paper, we provide an in-depth analysis of data preprocessing pipelines from four different machine learning domains. We introduce a new perspective on efficiently preparing datasets for end-to-end deep learning pipelines and extract individual trade-offs to optimize throughput, preprocessing time, and storage consumption. Additionally, we provide an open-source profiling library that can automatically decide on a suitable preprocessing strategy to maximize throughput. By applying our generated insights to real-world use-cases, we obtain an increased throughput of 3x to 13x compared to an untuned system while keeping the pipeline functionally identical. These findings show the enormous potential of data pipeline tuning.},
  booktitle = {Proceedings of the 2022 International Conference on Management of Data},
  pages     = {1825–1839},
  numpages  = {15},
  keywords  = {data processing, datasets, preprocessing, machine learning, deep learning},
  location  = {Philadelphia, PA, USA},
  series    = {SIGMOD '22}
}

@article{ramesh2021zero,
  author     = {Aditya Ramesh and
                Mikhail Pavlov and
                Gabriel Goh and
                Scott Gray and
                Chelsea Voss and
                Alec Radford and
                Mark Chen and
                Ilya Sutskever},
  title      = {Zero-Shot Text-to-Image Generation},
  journal    = {CoRR},
  volume     = {abs/2102.12092},
  year       = {2021},
  url        = {https://arxiv.org/abs/2102.12092},
  eprinttype = {arXiv},
  eprint     = {2102.12092},
  timestamp  = {Tue, 02 Mar 2021 12:11:01 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2102-12092.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{ryabinin2020learning,
  author     = {Maksim Riabinin and
                Anton Gusev},
  title      = {Learning@home: Crowdsourced Training of Large Neural Networks using
                Decentralized Mixture-of-Experts},
  journal    = {CoRR},
  volume     = {abs/2002.04013},
  year       = {2020},
  url        = {https://arxiv.org/abs/2002.04013},
  eprinttype = {arXiv},
  eprint     = {2002.04013},
  timestamp  = {Wed, 12 Feb 2020 16:38:55 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2002-04013.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{ryabinin2021mosphit,
  author     = {Max Ryabinin and
                Eduard Gorbunov and
                Vsevolod Plokhotnyuk and
                Gennady Pekhimenko},
  title      = {Moshpit {SGD:} Communication-Efficient Decentralized Training on Heterogeneous
                Unreliable Devices},
  journal    = {CoRR},
  volume     = {abs/2103.03239},
  year       = {2021},
  url        = {https://arxiv.org/abs/2103.03239},
  eprinttype = {arXiv},
  eprint     = {2103.03239},
  timestamp  = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2103-03239.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{shazeer2017outrageously,
  author     = {Noam Shazeer and
                Azalia Mirhoseini and
                Krzysztof Maziarz and
                Andy Davis and
                Quoc V. Le and
                Geoffrey E. Hinton and
                Jeff Dean},
  title      = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
                Layer},
  journal    = {CoRR},
  volume     = {abs/1701.06538},
  year       = {2017},
  url        = {http://arxiv.org/abs/1701.06538},
  eprinttype = {arXiv},
  eprint     = {1701.06538},
  timestamp  = {Mon, 13 Aug 2018 16:46:11 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/ShazeerMMDLHD17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{hivemind,
  author       = {Learning{@}home team},
  title        = {{H}ivemind: a {L}ibrary for {D}ecentralized {D}eep {L}earning},
  year         = {2020},
  howpublished = {\url{https://github.com/learning-at-home/hivemind}}
}

@article{xin2021production,
  author     = {Doris Xin and
                Hui Miao and
                Aditya G. Parameswaran and
                Neoklis Polyzotis},
  title      = {Production Machine Learning Pipelines: Empirical Analysis and Optimization
                Opportunities},
  journal    = {CoRR},
  volume     = {abs/2103.16007},
  year       = {2021},
  url        = {https://arxiv.org/abs/2103.16007},
  eprinttype = {arXiv},
  eprint     = {2103.16007},
  timestamp  = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2103-16007.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{devlin2018bert,
  author     = {Jacob Devlin and
                Ming{-}Wei Chang and
                Kenton Lee and
                Kristina Toutanova},
  title      = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
                Understanding},
  journal    = {CoRR},
  volume     = {abs/1810.04805},
  year       = {2018},
  url        = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint     = {1810.04805},
  timestamp  = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{brown2020gpt3,
  author     = {Tom B. Brown and
                Benjamin Mann and
                Nick Ryder and
                Melanie Subbiah and
                Jared Kaplan and
                Prafulla Dhariwal and
                Arvind Neelakantan and
                Pranav Shyam and
                Girish Sastry and
                Amanda Askell and
                Sandhini Agarwal and
                Ariel Herbert{-}Voss and
                Gretchen Krueger and
                Tom Henighan and
                Rewon Child and
                Aditya Ramesh and
                Daniel M. Ziegler and
                Jeffrey Wu and
                Clemens Winter and
                Christopher Hesse and
                Mark Chen and
                Eric Sigler and
                Mateusz Litwin and
                Scott Gray and
                Benjamin Chess and
                Jack Clark and
                Christopher Berner and
                Sam McCandlish and
                Alec Radford and
                Ilya Sutskever and
                Dario Amodei},
  title      = {Language Models are Few-Shot Learners},
  journal    = {CoRR},
  volume     = {abs/2005.14165},
  year       = {2020},
  url        = {https://arxiv.org/abs/2005.14165},
  eprinttype = {arXiv},
  eprint     = {2005.14165},
  timestamp  = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{peters2018elmo,
  author     = {Matthew E. Peters and
                Mark Neumann and
                Mohit Iyyer and
                Matt Gardner and
                Christopher Clark and
                Kenton Lee and
                Luke Zettlemoyer},
  title      = {Deep contextualized word representations},
  journal    = {CoRR},
  volume     = {abs/1802.05365},
  year       = {2018},
  url        = {http://arxiv.org/abs/1802.05365},
  eprinttype = {arXiv},
  eprint     = {1802.05365},
  timestamp  = {Mon, 13 Aug 2018 16:48:54 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1802-05365.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{radford2019language,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year   = {2019}
}

@article{shoeybi2019megatronlm,
  author     = {Mohammad Shoeybi and
                Mostofa Patwary and
                Raul Puri and
                Patrick LeGresley and
                Jared Casper and
                Bryan Catanzaro},
  title      = {Megatron-LM: Training Multi-Billion Parameter Language Models Using
                Model Parallelism},
  journal    = {CoRR},
  volume     = {abs/1909.08053},
  year       = {2019},
  url        = {http://arxiv.org/abs/1909.08053},
  eprinttype = {arXiv},
  eprint     = {1909.08053},
  timestamp  = {Tue, 24 Sep 2019 11:33:51 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1909-08053.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@misc{microsoft2020turingnlg,
  author       = {Microsoft},
  title        = {Turing-NLG: A 17-billion-parameter language model by Microsoft - Microsoft Research},
  howpublished = {\url{https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/}},
  month        = {05},
  year         = {2020}
}

@article{raffael2019t5,
  author     = {Colin Raffel and
                Noam Shazeer and
                Adam Roberts and
                Katherine Lee and
                Sharan Narang and
                Michael Matena and
                Yanqi Zhou and
                Wei Li and
                Peter J. Liu},
  title      = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
                Transformer},
  journal    = {CoRR},
  volume     = {abs/1910.10683},
  year       = {2019},
  url        = {http://arxiv.org/abs/1910.10683},
  eprinttype = {arXiv},
  eprint     = {1910.10683},
  timestamp  = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{smith2022megatronturingnlg,
  author     = {Shaden Smith and
                Mostofa Patwary and
                Brandon Norick and
                Patrick LeGresley and
                Samyam Rajbhandari and
                Jared Casper and
                Zhun Liu and
                Shrimai Prabhumoye and
                George Zerveas and
                Vijay Korthikanti and
                Elton Zheng and
                Rewon Child and
                Reza Yazdani Aminabadi and
                Julie Bernauer and
                Xia Song and
                Mohammad Shoeybi and
                Yuxiong He and
                Michael Houston and
                Saurabh Tiwary and
                Bryan Catanzaro},
  title      = {Using DeepSpeed and Megatron to Train Megatron-Turing {NLG} 530B,
                {A} Large-Scale Generative Language Model},
  journal    = {CoRR},
  volume     = {abs/2201.11990},
  year       = {2022},
  url        = {https://arxiv.org/abs/2201.11990},
  eprinttype = {arXiv},
  eprint     = {2201.11990},
  timestamp  = {Wed, 02 Feb 2022 15:00:01 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2201-11990.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{alexnet2012,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  url       = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
  volume    = {25},
  year      = {2012}
}

@inbook{Rumelhart1986learning,
  author    = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
  title     = {Learning Internal Representations by Error Propagation},
  year      = {1986},
  isbn      = {026268053X},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1: Foundations},
  pages     = {318–362},
  numpages  = {45}
}

@article{karras2019stylegan2,
  author     = {Tero Karras and
                Samuli Laine and
                Miika Aittala and
                Janne Hellsten and
                Jaakko Lehtinen and
                Timo Aila},
  title      = {Analyzing and Improving the Image Quality of StyleGAN},
  journal    = {CoRR},
  volume     = {abs/1912.04958},
  year       = {2019},
  url        = {http://arxiv.org/abs/1912.04958},
  eprinttype = {arXiv},
  eprint     = {1912.04958},
  timestamp  = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1912-04958.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
